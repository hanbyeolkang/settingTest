x-airflow-environment: &airflow-common-env
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: "false"
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow

  # Airflow Admin User
  _AIRFLOW_WWW_USER_USERNAME: 'admin'
  _AIRFLOW_WWW_USER_PASSWORD: 'admin'

  # Docker 설정
  DOCKER_HOST: unix:///var/run/docker.sock

  # dbt 프로젝트 경로
  # DBT_PROJECT_PATH: /Users/kang/settingTest/settingTest2/dbt  # Mac (본인 경로로 수정)
  DBT_PROJECT_PATH: /home/ec2-user/settingTest/dbt         # EC2

  # ENV
  AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
  AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
  AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
  S3_BUCKET_NAME: ${S3_BUCKET_NAME}
  REDSHIFT_HOST: ${REDSHIFT_HOST}
  REDSHIFT_PORT: ${REDSHIFT_PORT}
  REDSHIFT_DB: ${REDSHIFT_DB}
  REDSHIFT_USER: ${REDSHIFT_USER}
  REDSHIFT_PASSWORD: ${REDSHIFT_PASSWORD}
  SEOUL_KEY: ${SEOUL_KEY}

services:
  # PostgreSQL for Airflow metadata
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  # Airflow init
  airflow-init:
    build: ./airflow
    user: "0:0"  # root로 실행
    entrypoint: >
      bash -c "
        echo 'Creating directories and setting permissions...' &&
        mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins &&
        chown -R 50000:50000 /opt/airflow/logs &&
        chmod -R 755 /opt/airflow/dags /opt/airflow/plugins &&
        echo 'Initializing Airflow Metadata DB...' &&
        airflow db init &&
        echo 'Creating admin user...' &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
        echo 'Airflow DB initialization completed.'
      "
    environment:
      <<: *airflow-common-env
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"

  # Airflow Webserver
  airflow-webserver:
    build: ./airflow
    command: ["airflow", "webserver"]
    ports:
      - "8080:8080"
    environment:
      <<: *airflow-common-env
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    # user: "50000:0"       # Mac
    user: "50000:992"   # EC2

  # Airflow Scheduler
  airflow-scheduler:
    build: ./airflow
    command: ["airflow", "scheduler"]
    environment:
      <<: *airflow-common-env
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    # user: "50000:0"       # Mac
    user: "50000:992"   # EC2

  # dbt
  dbt-runner:
    build:
      context: ./dbt
      dockerfile: Dockerfile
    image: dbt-runner:latest
    working_dir: /usr/app
    volumes:
      - ./dbt:/usr/app
    command: tail -f /dev/null
    # dbt-runner는 상시 실행할 필요 없음. DockerOperator가 image만 필요로 함.
    restart: "no"

volumes:
  postgres-db-volume: